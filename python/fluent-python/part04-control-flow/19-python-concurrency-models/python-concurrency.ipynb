{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python Concurrency\n",
    "- Concurrency is about **dealing** with lots of things at once.\n",
    "- Parallelism is about **doing** lots of things at once.\n",
    "\n",
    "We will study simple examples to introduce and compare Python’s <span style=\"color:skyblue\">*core packages for concurrent programming: `threading`, `multi processing`*</span>, and `asyncio` which shows <span style=\"color:lightgreenskyblue\">*Python’s three approaches to concurrency: threads, processes, and native coroutines*</span>.\n",
    "\n",
    "We will also look into high-level overviews of third-party <span style=\"color:skyblue\">*tools, libraries, application servers, and distributed task queues—all of which can enhance the performance and scalability of Python applications*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture\n",
    "\n",
    "Why concurrent programming is hard?\n",
    "- <span style=\"color:orange\">*Starting threads or processes is easy enough, but how do you keep track of them?*</span>. When we start a thread or process, we don’t automatically know when it’s done, and getting back results or errors requires setting up some communication channel, such as a <span style=\"color:lightgreen\">*message queue*</span>\n",
    "- <span style=\"color:orange\">*Starting a thread or a process is not cheap*</span>, so we don't want to start one of them just to perform a single computation and quit. Often we want to amortize the startup cost by <span style=\"color:lightgreen\">*making each thread or process into a “worker” that enters a loop and stands by for inputs to work on (so not shut down and start them all the time)*</span>.\n",
    "- <span style=\"color:orange\">*How do you make a worker quit when you don't need it anymore, without interrupting a job partway and leaving half-processed data / unreleased resources (e.g. open files)?*</span> -> <span style=\"color:lightgreen\">*message queue*</span>\n",
    "- <span style=\"color:skyblue\">*A coroutine is cheap to start. If you start a coroutine using the `await` keyword, it’s easy to get a value returned by it, it can be safely cancelled, and you have a clear site to catch exceptions*</span>. <span style=\"color:orange\">*But coroutines are often started by the asynchronous framework, and that can make them as hard to monitor as threads or processes. Finally, Python coroutines and threads are not suitable for CPU-intensive tasks, as we’ll see.*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency Terms\n",
    "\n",
    "- <span style=\"color:skyblue\">**Concurrency**</span>: <span style=\"color:skyblue\">*The ability to handle multiple pending tasks, making progress one at a time or in parallel (if possible) so that each of them eventually succeeds or fails*</span>. A single core CPU is capable of concurrency by interleaving the execution of pending tasks\n",
    "- <span style=\"color:skyblue\">**Parallelism**</span>: <span style=\"color:skyblue\">*The ability to execute multiple computations at the same time*</span>. This requires a multicore CPU, multiple CPUs, a GPU, or multiple computers in a cluster\n",
    "- <span style=\"color:skyblue\">**Execution unit**</span>: <span style=\"color:skyblue\">*Objects that execute code concurrently, each with independens state and call stack*</span>. *Python natively supports three kinds of execution units: processes, threads, and coroutines*.\n",
    "- <span style=\"color:skyblue\">**Process**</span>: <span style=\"color:skyblue\">*An instance of a computer program while it is running, using memory and a slice of the CPU time*</span>. Processes communicate via pipes, sockets, or memory mapped files—all of which can only carry raw bytes. <span style=\"color:orange\">*Python objects must be serialized (converted) into raw bytes to pass from one process to another*</span>. Processes allow *preemptive multitasking*: the OS scheduler preempts—i.e., suspends - each running process periodically to allow other processes to run\n",
    "- <span style=\"color:skyblue\">**Thread**</span>: <span style=\"color:skyblue\">*An execution unit within a single process. When a process starts, it uses a single thread: the main thread*</span>. A process can create more threads to operate concurrently by calling operating system APIs. Threads within a process share the same memory space, which holds live Python objects -> easy data sharing, but can lead to corrupted data. Like processes, threads also enable *preemptive multitasking* under the supervision of the OS scheduler.\n",
    "- <span style=\"color:skyblue\">**Coroutine**</span>: <span style=\"color:skyblue\">*A coroutine is a function that can suspend itself and resume later*</span>. In Python, classic coroutines are built from generator functions, and native coroutines are defined with `async def`. Python coroutines usually run within a single thread under the supervision of an event loop, also in the same thread. Coroutines support *cooperative multitasking*: each coroutine must explicitly cede control with the yield or await keyword, so that another may proceed concurrently (but not in parallel).\n",
    "- <span style=\"color:skyblue\">**Queue**</span>: <span style=\"color:skyblue\">*A data structure that lets us put and get items, usually in FIFO order: first in, first out*</span>. The `queue` package in Python’s std is a FIFO and provides queue classes to support threads. \n",
    "- <span style=\"color:skyblue\">**Lock**</span>: <span style=\"color:skyblue\">*An object that execution units can use to synchronize their actions and avoid corrupting data*</span>. While updating a shared data structure, the running code should hold an associated lock. This signals other parts of the program to wait until the lock is released before accessing the same data structure.\n",
    "- <span style=\"color:skyblue\">**Contention**</span>: Resource contention happens when multiple execution units try to access a shared resource—such as a lock or storage. CPU contention happens when compute-intensive processes or threads must wait for the OS scheduler to give them a share of the CPU time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes, Threads, and Python’s Infamous GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Each instance of the Python interpreter is a process*. You can start additional Python processes using the `multiprocessing` or concurrent`.futures` libraries.\n",
    "2. *The Python interpreter uses a single thread to run the user’s program and the memory garbage collector*. You can start additional Python threads using the `threading` or `concurrent.futures` libraries.\n",
    "3. *Access to object reference counts and other internal interpreter state is controlled by a lock, the Global Interpreter Lock (GIL). Only one Python thread can hold the GIL at any time. This means that only one thread can execute Python code at any time, regardless of the number of CPU cores.*\n",
    "4. To prevent a Python thread from holding the GIL indefinitely, *Python’s bytecode interpreter pauses the current Python thread every 5ms by defaul to release the GIL*.\n",
    "5. When we write Python code, *we have no control over the GIL*.\n",
    "6. Every Python standard library function that makes a `syscall` releases the GIL. *syscall is a call from user code to a function of the operating system kernel*. This includes all functions that perform disk I/O, network I/O, and `time.sleep()`\n",
    "7. Extensions that integrate at the Python/C API level can also launch other non-Python threads that are not affected by the GIL.\n",
    "8. *The effect of the GIL on network programming with Python threads is relatively small, because the I/O functions release the GIL*, and reading or writing  to the network always implies high latency—compared to reading and writing to memory.\n",
    "9. *Contention over the GIL slows down compute-intensive Python threads*. Sequential, single-threaded code is simpler and faster for such tasks.\n",
    "10. *To run CPU-intensive Python code on multiple cores, you must use multiple Python processes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Concurrent Python Hello World\n",
    "\n",
    "***Idea***: We will make 2 functions, `spin` and `slow` that run concurrently. *The main thread — the only thread when the program starts — will start a new thread to run `spin` and then call `slow`*.  \n",
    "`slow` start a function that blocks for 3 seconds while animating characters in the terminal to let the user know that the program is “thinking” (which shows `\"\\|/-\"`) and not stalled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Threads\n",
    "\n",
    "We use `threading.Event` to coordinate threads. It has an internal `flag` which is set to `False` first.\n",
    "- Call `Event.set()` sets the flag to `True`\n",
    "- While the `flag` is false, if a thread calls `Event.wait()`, it is blocked until another thread calls `Event.set()`, at which time `Event.wait()` returns `True`.\n",
    "- If a timeout in seconds is given to `Event.wait(s)`, this call returns `False` when the timeout elapses, or returns `True` as soon as `Event.set()` is called by another thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from threading import Thread, Event\n",
    "\n",
    "def spin(msg: str, done: Event) -> None:  \n",
    "    \"\"\"\n",
    "    This function will run in a separate thread. \n",
    "\n",
    "    Args:\n",
    "        msg (str): the message to be printed\n",
    "        done (threading.Event): a simple object to synchronize threads\n",
    "    \"\"\"\n",
    "    for char in itertools.cycle(r'\\|/-'):  # inf loop as itertools.cycle yields one character at a time, cycling through the string forever.\n",
    "        status = f'\\r{char} {msg} '  # \\r move the cursor back to the start of the line\n",
    "        print(status, end='', flush=True)\n",
    "        if done.wait(.1):  # used to receive signal to break from the loop from the main thread\n",
    "            break  # break from the loop\n",
    "    \n",
    "    blanks = ' ' * len(status)  # clear the Clear the status line\n",
    "    print(f'\\r{blanks}\\r', end='')\n",
    "\n",
    "\n",
    "def slow(num_sec: int = 3) -> int:\n",
    "    for i in range(num_sec):\n",
    "        time.sleep(1)\n",
    "        print(f\"also doing work in the main thread for {i+1} (sec)\")\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Thread(Thread-5 (spin), initial)>\n",
      "| spinning in the secondary thread! also doing work in the main thread for 1 (sec)\n",
      "- spinning in the secondary thread! also doing work in the main thread for 2 (sec)\n",
      "| spinning in the secondary thread! also doing work in the main thread for 3 (sec)\n",
      "Answer: 42                           \n"
     ]
    }
   ],
   "source": [
    "def supervisor() -> int:  # supervisor will return the result of slow\n",
    "    done = Event()  # this key key to coordinate the activities of the main thread and the spinner thread\n",
    "    spinner = Thread(target=spin, args=('spinning in the secondary thread!', done))  # the spinner thread\n",
    "    print(f'spinner object: {spinner}')  # should output <Thread(Thread-1, initial)>\n",
    "    spinner.start()  # Start the spinner thread\n",
    "    result = slow()  # Call slow, which runs in the main thread and blocks it the main thread\n",
    "                     # Meanwhile, the secondary thread is running the spinner animation\n",
    "    done.set()  # Set the Event flag to True to terminate the for loop inside the spin function\n",
    "    spinner.join()  # Wait until the spinner thread finishes\n",
    "    return result\n",
    "\n",
    "def main() -> None:\n",
    "    result = supervisor()\n",
    "    print(f'Answer: {result}')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Processes\n",
    "<span style=\"color:skyblue\">*The `multiprocessing` package supports running concurrent tasks in separate Python processes instead of threads.  When you create a `multiprocessing.Process` instance, a whole new Python interpreter is started as a child process in the background*</span>. Since each Python process has its own GIL, this allows your program to use all available CPU cores and run in a truly paralell manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from multiprocessing import Process, Event\n",
    "from multiprocessing import synchronize\n",
    "\n",
    "def spin(msg: str, done: synchronize.Event) -> None:  # only difference is here compared to the threading version\n",
    "    \"\"\"\n",
    "    This function will run in a separate thread. \n",
    "\n",
    "    Args:\n",
    "        msg (str): the message to be printed\n",
    "        done (threading.Event): a simple object to synchronize threads\n",
    "    \"\"\"\n",
    "    for char in itertools.cycle(r'\\|/-'):  # inf loop as itertools.cycle yields one character at a time, cycling through the string forever.\n",
    "        status = f'\\r{char} {msg} '  # \\r move the cursor back to the start of the line\n",
    "        print(status, end='', flush=True)\n",
    "        if done.wait(.1):  # used to receive signal to break from the loop from the main thread\n",
    "            break  # break from the loop\n",
    "    \n",
    "    blanks = ' ' * len(status)  # clear the Clear the status line\n",
    "    print(f'\\r{blanks}\\r', end='')\n",
    "\n",
    "\n",
    "def slow(num_sec: int = 3) -> int:\n",
    "    for i in range(num_sec):\n",
    "        time.sleep(1)\n",
    "        print(f\"-- also doing work in the main thread of the main process for {i+1} (sec)\")\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Process name='Process-2' parent=67180 initial>\n",
      "- spinning in a child process! -- also doing work in the main thread of the main process for 1 (sec)\n",
      "| spinning in a child process! -- also doing work in the main thread of the main process for 2 (sec)\n",
      "-- also doing work in the main thread of the main process for 3 (sec)\n",
      "Answer: 42\n"
     ]
    }
   ],
   "source": [
    "def supervisor() -> int:\n",
    "    done = Event()\n",
    "    spinner = Process(target=spin, args=('spinning in a child process!', done))  # Basic usage of the `Process` class is similar to `Thread`\n",
    "    print(f'spinner object: {spinner}')  # should output something like <Process name='Process-1' parent=237132 initial>\n",
    "    spinner.start()\n",
    "    result = slow()\n",
    "    done.set()\n",
    "    spinner.join()\n",
    "    return result\n",
    "\n",
    "def main() -> None:\n",
    "    result = supervisor()\n",
    "    print(f'Answer: {result}')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic API of `threading` and `multiprocessing` are similar, but their implementation is very different, and `multiprocessing` has a much larger API to handle the added complexity of multiprocess programming. For example, one challenge when converting from threads to processes is how to communicate between processes that are isolated by the operating system and can’t share Python objects, and we have to serialize and deserialize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Coroutines\n",
    "\n",
    "<span style=\"color:skyblue\">*It is the job of OS schedulers to allocate CPU time to drive threads and processes. In contrast, coroutines are driven by an application-level event loop*</span> that manages a queue of pending coroutines, drives them one by one, monitors events triggered by I/O operations initiated by coroutines, and passes control back to the corresponding coroutine when each event happens. <span style=\"color:skyblue\">*The event loop and the library coroutines and the user coroutines all execute in a **single thread***</span>. Therefore, any time spent in a coroutine slows down the event loop—and all other coroutines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into the code of `spinner_async.py` to see how spinner with coroutines work since in jupyter notebook, we are already running in an async context with Jupyter's running event loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import asyncio\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "async def spin(msg: str) -> None:  # We don’t need the Event argument that was used to signal that\n",
    "                                   # `slow` had completed its job like in thread or process versions\n",
    "    for char in itertools.cycle(r'\\|/-'):\n",
    "        status = f'\\r{char} {msg}'\n",
    "        print(status, flush=True, end='')\n",
    "        try:\n",
    "            await asyncio.sleep(.1)  # Use `await asyncio.sleep(.1)` instead of `time.sleep(.1)`, to pause without blocking other coroutines\n",
    "        except asyncio.CancelledError:  # `asyncio.CancelledError` is raised when the cancel method is called on the `Task` controlling this coroutine\n",
    "            break\n",
    "    blanks = ' ' * len(status)\n",
    "    print(f'\\r{blanks}\\r', end='')\n",
    "\n",
    "async def slow() -> int:\n",
    "    await asyncio.sleep(3)  # also uses `await asyncio.sleep` instead of `time.sleep` to pause without blocking other coroutines\n",
    "    # time.sleep(3)  # experiment: with time.sleep, we will not see the spin at all\n",
    "    return 42\n",
    "\n",
    "async def supervisor() -> int:  # Native coroutines are defined with async def\n",
    "    spinner = asyncio.create_task(spin('thinking!'))  # `asyncio.create_task` schedules the eventual execution\n",
    "                                                      # of `spin`, immediately returning an instance of `asyncio.Task`\n",
    "    print(f'spinner object: {spinner}')  # should print an `asyncio.Task` object\n",
    "    result = await slow()  # The `await` keyword calls `slow`, blocking supervisor until `slow` returns\n",
    "    spinner.cancel()  # The `Task.cancel` method raises a `CancelledError` exception inside the `spin` coroutine\n",
    "    return result\n",
    "\n",
    "def main() -> None:  # since we are running in the jupyter notebook, which is already an async context, \n",
    "                           # we have to make main an async function and await for it\n",
    "    result = asyncio.run(supervisor())\n",
    "    # The `asyncio.run` function starts the event loop to drive \n",
    "    # the coroutine that will eventually set the other coroutines \n",
    "    # in motion. The main function will stay blocked until supervisor returns\n",
    "    # `supervisor`'s return value will be the return value of `asyncio.run`.\n",
    "    print(f'-- Answer: {result}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Task pending name='Task-2' coro=<spin() running at /home/dk/Desktop/projects/programming/python/fluent-python/part04-control-flow/19-python-concurrency-models/./spinner_async.py:5>>\n",
      "-- Answer: 42\n"
     ]
    }
   ],
   "source": [
    "!python ./spinner_async.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the 3 main ways of running a coroutine:\n",
    "- `asyncio.run(coro())`: Called from a regular function to drive a coroutine object that usually is the entry point for all the asynchronous code in the program, like the `supervisor` in this example. This call blocks until the body of `coro` returns.\n",
    "- `asyncio.create_task(coro())`: Called from a coroutine to schedule another coroutine to execute eventually. This call does not suspend the current coroutine.\n",
    "- `await coro()`: Called from a coroutine to transfer control to the coroutine object returned by `coro()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution flows of `spinner_async.py` is as follow:\n",
    "\n",
    "1. When `main()` is called, it calls `asyncio.run(supervisor())`\n",
    "2. `supervisor()` starts by creating a task for the `spin` coroutine and `awaits` the result of the `slow` coroutine.\n",
    "3. While `slow` is waiting (using `await asyncio.sleep(3)`), `spin` continues to run concurrently, printing the spinning animation.\n",
    "4. After 3 seconds, `slow` completes, and its `result` (`42`) is returned. Then, `spinner.cancel()` is called to cancel the spinning animation.\n",
    "5. Finally, the result is printed in `main()`\n",
    "\n",
    "<span style=\"color:lightgreen\">*Since by default, `asyncio` has only one flow of execution => only one coroutine can execute at a given point in time. When `asyncio.sleep()` is used instead of `time.sleep()`, the event loop is able to switch between tasks efficiently, allowing both coroutines (`spin` and `slow`) to run concurrently without blocking each other. If we use `time.sleep`, it will block the main thread's event loop so it will not be able to switch to the `spin` coroutine, hence, we will not be able to see the spinner at all*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greenlet and Gevent\n",
    "- **Greenlet**: The package supports cooperative multitasking through lightweight coroutines—named `greenlet` - that don’t require any special syntax such as `yield` or `await`, therefore are easier to integrate into existing, sequential codebases\n",
    "- **Gevent**: The `gevent` networking library monkey patches Python’s standard socket module making it nonblocking by replacing some of its code with greenlets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threaded vs Asyncio `supervisor`\n",
    "\n",
    "#### Threaded Version\n",
    "```python\n",
    "def supervisor() -> int:\n",
    "    done = Event()\n",
    "    spinner = Thread(target=spin,\n",
    "                     args=('thinking!', done))\n",
    "    print('spinner object:', spinner)\n",
    "    spinner.start()\n",
    "    result = slow()\n",
    "    done.set()\n",
    "    spinner.join()\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Async version\n",
    "```python\n",
    "async def supervisor() -> int:\n",
    "    spinner = asyncio.create_task(spin('thinking!'))\n",
    "    print('spinner object:', spinner)\n",
    "    result = await slow()\n",
    "    spinner.cancel()\n",
    "    return result\n",
    "```\n",
    "\n",
    "- An `asyncio.Task` is roughly the equivalent of a `threading.Thread`\n",
    "- A `Task` drives a coroutine object, and a `Thread` invokes a callable.\n",
    "- A coroutine `yields` control explicitly with the `await` keyword.\n",
    "- You don’t instantiate `Task` objects yourself, you get them by passing a coroutine to `asyncio.create_task(…)`.\n",
    "- When `asyncio.create_task(…)` returns a `Task` object, it is already scheduled to run, but a `Thread` instance must be explicitly told to run by calling its `start` method\n",
    "- In the threaded `supervisor`, `slow` is a plain function and is directly invoked by the main thread. In the asynchronous `supervisor`, `slow` is a coroutine driven by `await`.\n",
    "- There’s no API to terminate a thread from the outside; instead, you must send a signal—like setting the `done` `Event` object. For tasks, there is the `Task.cancel()` instance method, which raises `CancelledError` at the `await` expression where the coroutine body is currently suspended.\n",
    "- The `supervisor` coroutine must be started with `asyncio.run` in the main\n",
    "function.\n",
    "\n",
    "\n",
    "One final point related to threads versus coroutines: \n",
    "- If you’ve done any nontrivial programming with threads, you know how challenging it is to reason about the program because the scheduler can interrupt a thread at any time. You <span style=\"color:skyblue\">*must remember to hold locks to protect the critical sections of your program, to avoid getting interrupted in the middle of a multistep operation—which could leave data in an invalid state*</span>.\n",
    "- With coroutines, your code is protected against interruption by default. You must explicitly `await` to let the rest of the program run. <span style=\"color:skyblue\">*Instead of holding locks to synchronize the operations of multiple threads, coroutines are “synchronized” by definition: only one of them is running at any time. When you want to give up control, you use `await` to yield control back to the scheduler*</span>. That’s why it is possible to safely cancel a coroutine: by definition, a coroutine can only be cancelled when it’s suspended at an `await` expression, so you can perform cleanup by handling the `CanceledError` exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Impact of the GIL\n",
    "\n",
    "**For IO Intensive Code:**\n",
    "- We can replace `time.sleep` in the threaded `slow` function with an HTTP client request since <span style=\"color:skyblue\">*a well-designed network library will release the GIL while waiting for the network*</span>. \n",
    "- You can also replace the `asyncio.sleep(3)` expression in the `slow` coroutine to `await` for a response from <span style=\"color:skyblue\">*a well-designed asynchronous network library, because such libraries provide coroutines that yield control back to the event loop while waiting for the network*</span>. Meanwhile, the spinner will keep spinning.\n",
    "\n",
    "**With CPU-intensive code, the story is different.** For example, if we have an intensive CPU task like `is_prime` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    import math\n",
    "    root = math.isqrt(n)\n",
    "    for i in range(3, root + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "is_prime(5_000_111_000_222_021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we replace the `time.sleep(3)` (or `asyncio.sleep(3)`) in the `slow` function, what will happen?\n",
    "- **For the spinner with processes version**: Nothing happens as the spinner is controlled by a child process, so it continues spinning while the primality test is computed by the parent process\n",
    "- **For the threading spinner version**: The spinner is controlled by a secondary thread, so it continues spinning while the primality test is computed by the main thread. In this particular example, the spinner keeps spinning because Python suspends the running thread every 5ms (by default), making the GIL available to other pending threads. Therefore, <span style=\"color:skyblue\">*the main thread running `is_prime` is interrupted every 5ms, allowing the secondary thread to wake up and iterate once through the for loop, until it calls the `wait` method of the `done` event, at which time it will release the GIL. The main thread will then grab the GIL, and the `is_prime` computation will proceed for another 5ms.\n",
    "This does not have a visible impact on the running time of this specific example,\n",
    "because the `spin` function quickly iterates once and releases the GIL as it waits for the `done` event, so there is not much contention for the GIL. The main thread running `is_prime` will have the GIL most of the time.*</span>\n",
    "- **For the asyncio spinner version**: If you call `is_prime` in the `slow` coroutine of the `spinner_async.py` example, the spinner will never appear (similar to when we replaced `await asyncio.sleep(3)` with `time.sleep(3)`).   \n",
    "<span style=\"color:lightgreen\">*One way to keep the spinner alive is to rewrite `is_prime` as a coroutine, and periodically call `asyncio.sleep(0)` in an `await` expression to yield control back to the event loop so it can run the spinner. However, this will slow down the event loop and hence the whole program with it. Using `await asyncio.sleep(0)` should be considered a stopgap measure before you refactor your asynchronous code to delegate CPU-intensive computations to another process*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Task pending name='Task-2' coro=<spin() running at /home/dk/Desktop/projects/programming/python/fluent-python/part04-control-flow/19-python-concurrency-models/./spinner_async.py:6>>\n",
      "\\ spinning!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ spinning!\n",
      "5000111000222021 is a prime\n",
      "-- Answer: 42\n"
     ]
    }
   ],
   "source": [
    "!python ./spinner_async.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Homegrown Process Pool\n",
    "<span style=\"color:lightgreen\">*This chapter shows the use of multiple processes for CPU-intensive tasks, and the common pattern of using queues to distribute tasks and collect results*</span>. Chapter 20 will show a simpler way of distributing tasks to processes: a `ProcessPoolExecutor` from the `concurrent.futures` package, which uses queues internally.\n",
    "\n",
    "Example program: a program to check the primality of a sample of 20 integers from 1 to 10^16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20 numbers sequentially:\n",
      "               2 P  0.000001s\n",
      " 142702110479723 P  0.316831s\n",
      " 299593572317531 P  0.574896s\n",
      "3333333333333301 P  1.754009s\n",
      "3333333333333333    0.000005s\n",
      "3333335652092209    1.808153s\n",
      "4444444444444423 P  2.064743s\n",
      "4444444444444444    0.000001s\n",
      "4444444488888889    1.878283s\n",
      "5555553133149889    2.423506s\n",
      "5555555555555503 P  2.550794s\n",
      "5555555555555555    0.000007s\n",
      "6666666666666666    0.000000s\n",
      "6666666666666719 P  2.357989s\n",
      "6666667141414921    2.567975s\n",
      "7777777536340681    2.926401s\n",
      "7777777777777753 P  2.740157s\n",
      "7777777777777777    0.000006s\n",
      "9999999999999917 P  2.949505s\n",
      "9999999999999999    0.000005s\n",
      "Total time: 26.91s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from typing import NamedTuple\n",
    "import math\n",
    "\n",
    "PRIME_FIXTURE = [\n",
    "    (2, True),\n",
    "    (142702110479723, True),\n",
    "    (299593572317531, True),\n",
    "    (3333333333333301, True),\n",
    "    (3333333333333333, False),\n",
    "    (3333335652092209, False),\n",
    "    (4444444444444423, True),\n",
    "    (4444444444444444, False),\n",
    "    (4444444488888889, False),\n",
    "    (5555553133149889, False),\n",
    "    (5555555555555503, True),\n",
    "    (5555555555555555, False),\n",
    "    (6666666666666666, False),\n",
    "    (6666666666666719, True),\n",
    "    (6666667141414921, False),\n",
    "    (7777777536340681, False),\n",
    "    (7777777777777753, True),\n",
    "    (7777777777777777, False),\n",
    "    (9999999999999917, True),\n",
    "    (9999999999999999, False),\n",
    "]\n",
    "\n",
    "NUMBERS = [n for n, _ in PRIME_FIXTURE]\n",
    "\n",
    "# tag::IS_PRIME[]\n",
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    root = math.isqrt(n)\n",
    "    for i in range(3, root + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Result(NamedTuple):\n",
    "    prime: bool\n",
    "    elapsed: float\n",
    "\n",
    "def check(n: int) -> Result:\n",
    "    \"\"\"\n",
    "    calls is_prime(n) and computes the elapsed time to return a Result.\n",
    "    \"\"\"\n",
    "    t0 = perf_counter()\n",
    "    prime = is_prime(n)\n",
    "    return Result(prime, perf_counter() - t0)\n",
    "\n",
    "def main() -> None:\n",
    "    print(f'Checking {len(NUMBERS)} numbers sequentially:')\n",
    "    t0 = perf_counter()\n",
    "    for n in NUMBERS:\n",
    "        prime, elapsed = check(n)\n",
    "        label = 'P' if prime else ' '\n",
    "        print(f'{n:16} {label} {elapsed:9.6f}s')\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f'Total time: {elapsed:.2f}s')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process-Based Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20 numbers with 8 processes:\n",
      "               2  P  0.000006s\n",
      "3333333333333333     0.000014s\n",
      "4444444444444444     0.000005s\n",
      " 142702110479723  P  0.575875s\n",
      "5555555555555555     0.000007s\n",
      "6666666666666666     0.000001s\n",
      " 299593572317531  P  0.727143s\n",
      "3333333333333301  P  2.448605s\n",
      "4444444444444423  P  2.678654s\n",
      "5555555555555503  P  3.219040s\n",
      "7777777777777777     0.000007s\n",
      "3333335652092209     3.364439s\n",
      "9999999999999999     0.000007s\n",
      "4444444488888889     3.776165s\n",
      "6666667141414921     3.272632s\n",
      "5555553133149889     4.183812s\n",
      "6666666666666719  P  3.970167s\n",
      "7777777777777753  P  2.936584s\n",
      "7777777536340681     3.261300s\n",
      "9999999999999917  P  3.258965s\n",
      "20 checks in 6.51s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from typing import NamedTuple\n",
    "from multiprocessing import Process, SimpleQueue, cpu_count\n",
    "from multiprocessing import queues\n",
    "\n",
    "# tag::PRIMES_PROC_TOP[]\n",
    "class PrimeResult(NamedTuple):\n",
    "    \"\"\"\n",
    "    Includes the number checked for primality\n",
    "    \"\"\"\n",
    "    n: int\n",
    "    prime: bool\n",
    "    elapsed: float\n",
    "\n",
    "JobQueue = queues.SimpleQueue[int]  # type alias for a `SimpleQueue` that the `main` function\n",
    "                                    # will use to send numbers to the processes that will do the work\n",
    "ResultQueue = queues.SimpleQueue[PrimeResult]  # Type alias for a second `SimpleQueue` \n",
    "                                               # that will collect the results in `main`.\n",
    "\n",
    "def check(n: int) -> PrimeResult:\n",
    "    \"\"\"\n",
    "    Check prime and return PrimeResult for a number `n`\n",
    "    \"\"\"\n",
    "    t0 = perf_counter()\n",
    "    res = is_prime(n)\n",
    "    return PrimeResult(n, res, perf_counter() - t0)\n",
    "\n",
    "def worker(jobs: JobQueue, results: ResultQueue) -> None:\n",
    "    \"\"\"\n",
    "    Get the number to be checked from the `job` queue, check it\n",
    "    using the `check(n)` function, then use the `results` queue\n",
    "    to put the PrimeResult. Only stops if encounter a sentinel value \n",
    "    (the number to be checked is 0)\n",
    "    \"\"\"\n",
    "    while n := jobs.get():  # if n == 0, then signal for the worker to finish\n",
    "                            # otherwise loop indefinitely, taking items from\n",
    "                            # the job queue and process each item with the actual\n",
    "                            # function that does the work\n",
    "        results.put(check(n))  # Invoke the primality check and enqueue PrimeResult\n",
    "    results.put(PrimeResult(0, False, 0.0))  # send the number 0 to the main loop to signal that the worker is done\n",
    "\n",
    "def start_jobs(\n",
    "    num_processes: int, jobs: JobQueue, results: ResultQueue\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    First, put all the numbers to be check in the `job` queue\n",
    "    Then luanch `num_processes` child processes, each process\n",
    "    runs the worker function, \n",
    "    \"\"\"\n",
    "    for n in NUMBERS:\n",
    "        jobs.put(n)  # Enqueue the numbers to be checked in jobs\n",
    "    for _ in range(num_processes):\n",
    "        # Below, we fork a child process for each worker. Each child will run\n",
    "        # the loop inside its own instance of the worker function, \n",
    "        # until it fetches a 0 from the jobs queue.\n",
    "        process = Process(target=worker, args=(jobs, results))\n",
    "        process.start()  # Start each child process\n",
    "        jobs.put(0)  # Enqueue one 0 for each process, to terminate them\n",
    "# end::PRIMES_PROC_TOP[]\n",
    "\n",
    "# tag::PRIMES_PROC_MAIN[]\n",
    "def main() -> None:\n",
    "    procs = cpu_count() - 4  # let's work with (total cores - 4) processes\n",
    "\n",
    "    print(f'Checking {len(NUMBERS)} numbers with {procs} processes:')\n",
    "    t0 = perf_counter()\n",
    "    jobs: JobQueue = SimpleQueue()\n",
    "    results: ResultQueue = SimpleQueue()\n",
    "    start_jobs(procs, jobs, results)  # Start proc processes to consume jobs and post results\n",
    "    checked = report(procs, results)  # Retrieve the results and display them\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f'{checked} checks in {elapsed:.2f}s')  # Display how many numbers were checked and the total elapsed time\n",
    "\n",
    "def report(procs: int, results: ResultQueue) -> int: # <6>\n",
    "    checked = 0\n",
    "    procs_done = 0\n",
    "    while procs_done < procs:  # Loop until all processes are done.\n",
    "        n, prime, elapsed = results.get()  # Get one PrimeResult. Calling `.get()` on a queue block until there is an item in the queue\n",
    "        if n == 0:  # If n is zero, then one process exited; increment the procs_done count\n",
    "            procs_done += 1\n",
    "        else:\n",
    "            # Otherwise, increment the checked count (to keep track of the \n",
    "            # numbers checked) and display the results\n",
    "            checked += 1\n",
    "            label = 'P' if prime else ' '\n",
    "            print(f'{n:16}  {label} {elapsed:9.6f}s')\n",
    "    return checked\n",
    "# end::PRIMES_PROC_MAIN[]\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread-Based Nonsolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the thread-based version which is very similar to the process-based solution, however, <span style=\"color:orange\">*due to the GIL and the compute-intensive nature of `is_prime`, the threaded version is almost slower than the sequential version, and it gets slower as the number of threads increase, because of CPU contention and the cost of context switching. To switch to a new thread, the OS needs to save CPU registers and update the program counter and stack pointer, triggering expensive side effects like invalidating CPU caches and possibly even swapping memory pages*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20 numbers with 8 threads:\n",
      "               2  P  0.000002s\n",
      "3333333333333333     0.000006s\n",
      "4444444444444444     0.000002s\n",
      " 299593572317531  P  1.932373s\n",
      "5555555555555555     0.000006s\n",
      "6666666666666666     0.000000s\n",
      " 142702110479723  P  2.136972s\n",
      "3333333333333301  P 12.423121s\n",
      "4444444444444423  P 12.386578s\n",
      "5555555555555503  P 14.281875s\n",
      "7777777777777777     0.000008s\n",
      "3333335652092209    16.243054s\n",
      "9999999999999999     0.000006s\n",
      "6666666666666719  P 15.059455s\n",
      "5555553133149889    16.386158s\n",
      "6666667141414921    15.597601s\n",
      "4444444488888889    18.118407s\n",
      "7777777536340681    10.222358s\n",
      "7777777777777753  P 10.575917s\n",
      "9999999999999917  P  8.389822s\n",
      "20 checks in 23.45s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from queue import SimpleQueue\n",
    "from time import perf_counter\n",
    "from typing import NamedTuple\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class PrimeResult(NamedTuple):\n",
    "    n: int\n",
    "    prime: bool\n",
    "    elapsed: float\n",
    "\n",
    "JobQueue = SimpleQueue[int]  # <4>\n",
    "ResultQueue = SimpleQueue[PrimeResult]  # <5>\n",
    "\n",
    "def check(n: int) -> PrimeResult:  # <6>\n",
    "    t0 = perf_counter()\n",
    "    res = is_prime(n)\n",
    "    return PrimeResult(n, res, perf_counter() - t0)\n",
    "\n",
    "def worker(jobs: JobQueue, results: ResultQueue) -> None:  # <7>\n",
    "    while n := jobs.get():  # <8>\n",
    "        results.put(check(n))  # <9>\n",
    "    results.put(PrimeResult(0, False, 0.0))\n",
    "\n",
    "def start_jobs(workers: int, jobs: JobQueue, results: ResultQueue) -> None:\n",
    "    for n in NUMBERS:  # <3>\n",
    "        jobs.put(n)\n",
    "    for _ in range(workers):\n",
    "        proc = Thread(target=worker, args=(jobs, results))  # <4>\n",
    "        proc.start()  # <5>\n",
    "        jobs.put(0)  # <6>\n",
    "\n",
    "def report(workers: int, results: ResultQueue) -> int:\n",
    "    checked = 0\n",
    "    workers_done = 0\n",
    "    while workers_done < workers:\n",
    "        n, prime, elapsed = results.get()\n",
    "        if n == 0:\n",
    "            workers_done += 1\n",
    "        else:\n",
    "            checked += 1\n",
    "            label = 'P' if prime else ' '\n",
    "            print(f'{n:16}  {label} {elapsed:9.6f}s')\n",
    "    return checked\n",
    "\n",
    "def main() -> None:\n",
    "    workers = os.cpu_count() - 4\n",
    "\n",
    "    print(f'Checking {len(NUMBERS)} numbers with {workers} threads:')\n",
    "    t0 = perf_counter()\n",
    "    jobs: JobQueue = SimpleQueue()\n",
    "    results: ResultQueue = SimpleQueue()\n",
    "    start_jobs(workers, jobs, results)\n",
    "    checked = report(workers, results)\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f'{checked} checks in {elapsed:.2f}s')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python in the Multicore World\n",
    "\n",
    "Python’s story started in the early 1990s, when CPUs were still getting exponentially faster at sequential code execution. There was no talk about multicore CPUs except in supercomputers back then. At the time, the decision to have a GIL was a no-brainer. The GIL makes the interpreter faster when running on a single core, and its implementation simpler. The GIL also makes it easier to write simple extensions through the Python/C API.  \n",
    "Despite the GIL, Python is thriving in applications that require concurrent or parallel execution, thanks to libraries and software architectures that work around the limitations of CPython.\n",
    "\n",
    "- **System Administration**: Python is widely used to manage large fleets of servers, routers, load balancers, and network-attached storage (NAS).\n",
    "- **Data Science**: Jupyter Lab & Notebook, Tensorflow / PyTorch, Dask\n",
    "- **Server-Side Web/Mobile Development**: Python is widely used in web applications and for the backend APIs supporting mobile applications.\n",
    "- **WSGI Application Servers**: WSGI—the Web Server Gateway Interface—is a standard API for a Python framework or application to receive requests from an HTTP server and send responses to it. WSGI application servers manage one or more processes running your application, maximizing the use of the available CPUs. \n",
    "<img src=\"../images/WSGI.png\" style=\"width: 80%;\">.  \n",
    "Clients connect to an HTTP server that delivers static files and routes other requests to the application server, which forks child processes to run the application code, leveraging multiple CPU cores. The WSGI API is the glue between the application server and the Python application code. The main point: all of these application servers can potentially use all CPU cores on the server by forking multiple Python processes to run traditional web apps written in good old sequential code in Django, Flask, Pyramid, etc. This explains why it’s been possible to earn a living as a Python web developer without ever studying the `threading`, `multiprocessing`, or `asyncio` modules: the application server handles concurrency transparently.\n",
    "- **ASGI — Asynchronous Server Gateway Interface**: WSGI is a synchronous API. It doesn’t support coroutines with `async/await` — the most efficient way to implement `WebSockets` or HTTP long polling in Python. The ASGI specification is a successor to WSGI, designed for asynchronous Python web frameworks such as aiohttp, Sanic, FastAPI, etc., as well as Django and Flask, which are gradually adding asynchronous functionality.\n",
    "- **Distributed Task Queues**: When the application server delivers a request to one of the Python processes running your code, your app needs to respond quickly: you want the process to be available to handle the next request as soon as possible. However, some requests demand actions that may take longer — for example, sending email or generating a PDF. That’s the problem that distributed task queues are designed to solve. Celery and RQ are the best known open source task queues with Python APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter, we learned Python’s three native concurrency programming models:\n",
    "- Threads, with the `threading` package\n",
    "- Processes, with `multiprocessing`\n",
    "- Asynchronous coroutines with `asyncio`\n",
    "\n",
    "We then explored the real impact of the GIL with an experiment: changing the spin‐\n",
    "ner examples to compute the primality of a large integer and observe the resulting\n",
    "behavior. This demonstrated graphically that: \n",
    "1. <span style=\"color:green\">*CPU-intensive functions must be avoided in `asyncio`, as they block the event loop.*</span>\n",
    "2. The `threaded` version of the experiment worked—despite the GIL—because Python periodically interrupts threads, and the example used only two threads: one doing compute-intensive work, and the other\n",
    "driving the animation only 10 times per second. \n",
    "3. The `multiprocessing` variant\n",
    "worked around the GIL, starting a new process just for the animation, while the main process did the primality check.\n",
    "\n",
    "The next example, computing several primes, highlighted the difference between `multiprocessing` and `threading`, proving that <span style=\"color:green\">*only processes allow Python to benefit from multicore CPUs. Python’s GIL makes threads worse than sequential code for heavy computations*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
